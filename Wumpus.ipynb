{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794a948d",
   "metadata": {},
   "source": [
    "# Devoir maison n° 2 Tech. Agents, résoudre le problème du Wumpus par QLearning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b48775",
   "metadata": {},
   "source": [
    "Pour modéliser le problème du Wumpus, nous allons créer une grille de taille 4x4 représentant le labyrinthe. Chaque case peut être vide, contenir un mur, un piège, un trésor ou le Wumpus lui-même. L'agent peut se déplacer vers le haut, le bas, la gauche ou la droite, mais ne peut pas traverser les murs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae46ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a244388",
   "metadata": {},
   "source": [
    "Les récompenses pour chaque action et chaque état sont les suivantes :\n",
    "\n",
    "    -10 pour les actions qui conduisent à une case vide ou visitée\n",
    "    -1 pour les actions qui conduisent à une case inconnue\n",
    "    -100 pour l'action qui conduit au Wumpus \n",
    "    100 pour l'action qui conduit au trésor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d13417b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = np.array([[-10, -1, -1, -10],\n",
    "                          [-1, -1, -1, -100],\n",
    "                          [-1, -1, -1, -10],\n",
    "                          [-10, -1, -1, 100]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cd970",
   "metadata": {},
   "source": [
    "La Q-table sera une matrice 4x4x4, où 4 est la taille de la grille et 4 représente les 4 actions possibles (haut, bas, gauche, droite). Chaque entrée de la Q-table représentera la valeur Q de l'état correspondant et de l'action correspondante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "941cb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la Q-table\n",
    "q_table = np.zeros((4, 4, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2d9b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des paramètres de l'algorithme\n",
    "alpha = 0.1 # taux d'apprentissage\n",
    "gamma = 0.9  # facteur de réduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0acef9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choix_action(state, q_table,epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:  # exploration\n",
    "        action = random.randint(0, 3)\n",
    "    else:  # exploitation\n",
    "        action = np.argmax(q_table[state[0], state[1], :])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69a712",
   "metadata": {},
   "source": [
    "Une fonction qui met à jour la Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab5f6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state, action, next_state, reward, alpha, gamma):\n",
    "    q_table[state[0], state[1], action] = (1 - alpha) * q_table[state[0], state[1], action] + alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1], :]))\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10d79f",
   "metadata": {},
   "source": [
    "L'algorithme Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09bc6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    # Initialisation de l'état de départ\n",
    "    state = [0, 0]\n",
    "    \n",
    "    # Boucle de simulation d'un épisode\n",
    "    while True:\n",
    "        # Choix de l'action\n",
    "        epsilon = 0.1  # probabilité d'exploration\n",
    "        action = choix_action(state, q_table, epsilon)\n",
    "        \n",
    "        # Application de l'action et observation du nouvel état et de la récompense\n",
    "        if action == 0:  # haut\n",
    "            next_state = [max(state[0] - 1, 0), state[1]]\n",
    "        elif action == 1:  # bas\n",
    "            next_state = [min(state[0] + 1, 3), state[1]]\n",
    "        elif action == 2:  # gauche\n",
    "            next_state = [state[0], max(state[1] - 1, 0)]\n",
    "        else:  # droite\n",
    "            next_state = [state[0], min(state[1] + 1, 3)]\n",
    "        \n",
    "        reward = reward_matrix[next_state[0], next_state[1]]\n",
    "        \n",
    "        # Mise à jour de la Q-table\n",
    "        q_table = update_q_table(q_table, state, action, next_state, reward, alpha, gamma)\n",
    "        \n",
    "        # Passage à l'état suivant\n",
    "        state = next_state\n",
    "        \n",
    "        # Vérification de l'état final\n",
    "        if reward == 100 or reward == -100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122e31b",
   "metadata": {},
   "source": [
    "L'affichage de la Q-table : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4738b5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  39.45851      54.9539       39.45851      54.9539    ]\n",
      "  [  32.8352873    62.171        27.75974952   49.45128943]\n",
      "  [  47.69167237   70.19         35.51191762    1.17492163]\n",
      "  [  -0.38406768  -10.           29.12739281    0.        ]]\n",
      "\n",
      " [[  39.45851      62.171        54.9539       62.171     ]\n",
      "  [  54.9539       70.19         54.9539       70.19      ]\n",
      "  [  62.171        79.1          62.171      -100.        ]\n",
      "  [   0.            0.            0.            0.        ]]\n",
      "\n",
      " [[  37.54078744   10.8660268    28.86698144   70.19      ]\n",
      "  [  47.87865528   66.75717171   56.81320478   79.1       ]\n",
      "  [  70.19         89.           70.19         79.99999999]\n",
      "  [ -40.951       100.           41.26671521   31.84818797]]\n",
      "\n",
      " [[  -0.19          3.02130765    2.61435548   63.71983459]\n",
      "  [  24.00689569   32.27472087   18.58928565   89.        ]\n",
      "  [  79.1          89.           79.1         100.        ]\n",
      "  [   0.            0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0f56f",
   "metadata": {},
   "source": [
    "## Analyse des résultats :     \n",
    "    \n",
    "    Cette Q-table correspond aux valeurs de Q pour chaque état et action possible. Elle est organisée en quatre matrices de taille 4x4, chacune représentant une position possible de l'agent dans l'environnement. On peut observer que certaines valeurs sont plus élevées que d'autres, ce qui signifie que l'agent a appris que certaines actions sont plus bénéfiques que d'autres dans certaines situations. Par exemple, les valeurs les plus élevées dans la première matrice se trouvent dans la colonne correspondant à la position la plus à gauche de l'agent, ce qui indique que l'agent a appris que cette position est plus favorable pour prendre certaines actions. Les valeurs les plus élevées dans chaque état : ces valeurs représentent les meilleures actions à prendre dans chaque état. Par exemple, dans l'état (0,1), la valeur la plus élevée est 62.171, ce qui signifie que l'agent devrait choisir l'action qui mène à l'état (1,1) pour maximiser sa récompense à long terme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
